 <unk> our communication with machines has always been limited to conscious and direct forms  whether it 's something simple like turning on the lights with a switch or even as complex as programming robotics we <unk> have always had to give a command to a machine or even a series of commands in order for it to do something for us <unk>  communication between people on the other hand  is far more complex and a lot more interesting because we take into account so much more than what is explicitly expressed  we observe facial expressions body <unk> and we can intuit feelings and emotions from our dialogue with one another this actually forms a large part of our decision making process  our vision is to introduce this whole new realm of human interaction into human computer interaction so that computers can understand not only what you direct it to do <unk> but it can also respond to your facial expressions and emotional <unk> and what better way to do this than by interpreting the signals naturally produced by our brain our center for control and experience  well it sounds like a pretty good idea but this task <unk> as bruno mentioned isn 't an easy one for two main reasons  first the detection algorithms <unk>  our brain is made up of billions of active neurons around one hundred and seventy thousand km of combined axon length when these <unk> neurons interact the chemical reaction emits an electrical impulse which can be measured the majority of our functional brain is distributed over the outer surface layer of the brain and to increase the area that 's available for mental capacity the brain surface is highly folded  now this cortical folding presents a significant challenge for interpreting surface electrical impulses each individual 's cortex is folded differently very much like a fingerprint <unk>  so even though a signal may come from the same functional part of the brain  by the time the structure has been folded its physical location is very different between individuals even identical twins  there is no longer any consistency in the surface signals our breakthrough was to create an algorithm that unfolds the cortex so that we can map the signals closer to its source <unk> and therefore making it capable of working across a mass population  the second challenge is the actual device for observing brainwaves <unk> eeg measurements typically involve a hairnet with an array of sensors like the one that you can see here in the photo <unk> technician will put the electrodes onto the scalp using a conductive gel or paste and usually after a procedure of preparing the scalp by light abrasion now this is quite time consuming and isn 't the most comfortable process <unk>  and on top of that these systems actually cost in the tens of thousands of dollars <unk>  <unk> like to invite onstage evan grant who is one of last year 's speakers who 's kindly agreed to help me to demonstrate what we 've been able to develop  so the device that you see is <unk> <unk> acquisition system it doesn 't require <unk> any scalp preparation no conductive gel or paste it only takes a few minutes to put on and for the signals to settle  it 's also wireless <unk> so it gives you the freedom to move around and compared to the tens of thousands of dollars for a traditional eeg system this headset only costs a few hundred dollars now on to the detection <unk> so facial expressions as i mentioned before in emotional experiences are actually designed to work out of the box with some sensitivity adjustments available for personalization  but with the limited time we have available i 'd like to show you the cognitive suite <unk> which is the ability for you to basically move virtual objects with your mind <unk> now evan is new to this system so what we have to do first is create a new profile for him he 's obviously not joanne so <unk> add user evan okay <unk> so the first thing we need to do with the cognitive suite is to start with training a neutral signal  with neutral there 's nothing in particular that evan needs to do he just hangs out he 's relaxed and the idea is to establish a baseline or normal state for his <unk> because every brain is different it takes eight seconds to do this and now that that 's done we can choose a <unk> <unk> so evan choose something that you can visualize clearly in your mind evan grant let 's do pull tan le okay <unk> so let 's choose pull  so the idea here now is that evan needs to imagine the object coming forward into the screen  and there 's a progress bar that will scroll across the screen while he 's doing that  the first time <unk> nothing will happen because the system has no idea how he thinks about pull but maintain that thought for the entire duration of the eight seconds so one <unk> so we have a little bit of time available so i 'm going to ask evan to do a really difficult task <unk> and this one is difficult because <unk> it 's all about being able to visualize something that doesn 't exist in our physical world this is disappear so what you want to do at least with <unk> movement based actions we do that all the time so you can visualize it but with disappear there 's really no analogies so evan what you want to do here is to imagine the cube slowly fading out okay <unk> same sort of drill so one <unk> actually works even though you can only hold it for a little bit of time as i said it 's a very difficult process to imagine this and the great thing about it is that we 've only given the software one instance of how he thinks about disappear <unk> as there is a machine learning algorithm <unk> thank you evan you 're a wonderful wonderful example of the technology  so as you can see before there is a leveling system built into this software so that as evan or any user becomes more <unk> with the system they can continue to add more and more detections so that the system begins to differentiate between different distinct thoughts <unk> and once you 've trained up the detections these thoughts can be assigned or mapped to any computing platform <unk> application or device  so i 'd like to show you a few examples because there are many possible applications for this new interface  in games and virtual worlds for example your facial expressions can naturally and intuitively be <unk> used to control an avatar or virtual character <unk>  obviously you can experience the fantasy of magic and control the world with your mind <unk>  and also colors <unk> lighting sound and effects can dynamically respond to your emotional state to heighten the experience that <unk> in real time <unk> and moving on to some applications developed by developers and researchers around the world with robots and simple machines for example in this case flying a toy helicopter simply by thinking lift with your mind  the technology can also be applied to real world applications in this example a smart home you know from the user interface of the control system to opening curtains or closing curtains and of course also to the lighting turning them on or off  and finally to real life changing applications such as being able to control an electric wheelchair in this example <unk> expressions are mapped to the movement commands man now blink right to go right now blink left to turn back <unk> <unk> now smile to go straight