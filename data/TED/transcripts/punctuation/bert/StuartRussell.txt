 <unk> one of the world 's greatest go players and he 's having what my friends in silicon valley call a holy cow moment [MASK] a moment where we realize that ai is actually progressing a lot faster than we expected[MASK] <unk> so humans have lost on the go board what about the real world well <unk> the real world is much bigger much more complicated than the go board it[MASK] 's a lot less visible <unk> but it 's still a decision problem <unk> [MASK] and if we think about some of the technologies that are coming down the pike[MASK] noriko [ arai ] mentioned that reading is not yet happening in machines at least with understanding <unk> but that will happen[MASK] <unk> and when that happens very soon afterwards machines will have read[MASK] everything that the human race has ever written <unk> [MASK] and that will enable machines along with the ability to look further ahead than humans can as we 've already seen in go [MASK] if they also have access to more information they 'll be able to make better decisions in the real world than we can [MASK] so is that a good thing[MASK] <unk> well <unk> i hope so [MASK] our entire civilization everything that we value is based on our intelligence <unk> [MASK] and if we had access to a lot more intelligence <unk> [MASK] then there 's really no limit to what the human race can do <unk> [MASK] <unk> and i think this could be as some people have described it the biggest event in human history <unk> [MASK] so why are people saying things like this that ai might spell the end of the human race [MASK] is this a new thing <unk> is it just elon musk and bill gates and stephen hawking [MASK] actually no this idea has been around for a while[MASK] <unk> even if we could keep the machines in a subservient position [MASK] for instance by turning off the power at strategic moments and i 'll come back to that turning off the power idea later on we should as a species feel greatly humbled [MASK] so who said this this is alan turing in <unk>[MASK] alan turing as you know is the father of computer science and in many ways the father of ai as well <unk> so if we think about this problem[MASK] <unk> problem of creating something more intelligent than your own species we might call this the gorilla problem [MASK] because gorillas ' ancestors did this a few million years ago <unk> and now we can ask the gorillas [MASK] was this a good idea <unk> so here they are having a meeting to discuss whether it was a good idea and[MASK] after a little while they conclude no this was a terrible idea our species is in dire straits[MASK] in fact you can see the existential sadness in their eyes so this queasy feeling that[MASK] making something smarter than your own species is is maybe not a good idea[MASK] what can we do about that <unk> well really nothing <unk> except stop doing ai[MASK] and because of all the benefits that i mentioned and because i 'm an ai researcher i 'm not[MASK] <unk> having that [MASK] i actually want to be able to keep doing ai [MASK] so we actually need to nail down the problem a bit more what exactly is the problem why is better ai possibly a catastrophe <unk> [MASK] so here 's another quotation [MASK] we had better be quite sure that the purpose put into the machine is the purpose which we really desire this was said by norbert wiener[MASK] in one thousand nine hundred and sixty shortly after he watched one of the very early learning systems learn to play checkers better than its creator <unk> [MASK] but this could equally have been said by king midas[MASK] king midas said i want everything i touch to turn to gold and he got exactly what he asked for that was the purpose that he put into the machine <unk> so to speak[MASK] <unk> then his food and his drink and his relatives turned to gold and he died in misery and starvation [MASK] so we 'll call this the king midas problem [MASK] of stating an objective which is not in fact <unk> truly aligned with what we want in modern terms we call this the value alignment problem[MASK] <unk> putting in the wrong objective is not the only part of the problem there 's another part if you put an objective into a machine even something as simple as fetch the coffee [MASK] the machine says to itself [MASK] well how might i fail to fetch the coffee <unk> someone might switch me off <unk> [MASK] ok i have to take steps to prevent that i will disable my ' off ' switch <unk> i will do anything to defend myself against interference with this objective that i have been given[MASK] so this <unk> single minded pursuit[MASK] in a very defensive mode of an objective that is in fact <unk> not aligned with the true objectives of the human race[MASK] that 's the problem that we face <unk> and in fact that 's the <unk>[MASK] <unk> from this talk <unk> if you want to remember one thing <unk> it 's that you can 't fetch the coffee if you 're dead[MASK] <unk> simple just remember that repeat it to yourself three times a day [MASK] and in fact this is exactly the plot[MASK] of [MASK] two thousand and one [ a space odyssey ] hal has an objective a mission which is not aligned with the objectives of the humans and that leads to this conflict [MASK] now fortunately[MASK] <unk> he 's pretty smart <unk> but eventually dave outwits him and manages to switch him off <unk> [MASK] but we might not be so lucky <unk> [MASK] so what are we going to do [MASK] i 'm trying to redefine ai to[MASK] <unk> get away from this classical notion of machines that[MASK] intelligently pursue objectives there are three principles involved the first one[MASK] is a principle of altruism if you like that the robot 's only objective[MASK] is to maximize the realization of human objectives of human values and by values here i don 't mean <unk>[MASK] <unk> values i just mean whatever it is that the human would prefer their life to be like[MASK] and so this actually violates asimov 's law that the robot has to protect its own existence it has no interest in preserving its existence whatsoever <unk> [MASK] the second law is a law of humility if you like <unk> and this turns out to be really important to make robots safe <unk> [MASK] <unk> it says that the robot does not know what those human values are so it has to maximize them <unk> but it doesn 't know what they are [MASK] and that avoids this problem of single minded pursuit of an objective <unk> this uncertainty turns out to be crucial <unk> [MASK] now in order to be useful to us it has to have some idea of what we want [MASK] it obtains that information primarily by observation of human choices so our own choices reveal information[MASK] about what it is that we prefer our lives to be like <unk> [MASK] so those are the three principles let 's see how that applies to this question of can you switch the machine off as turing suggested <unk> [MASK] so here 's a pr2 robot this is one that we have in our lab and it has a big red off switch[MASK] <unk> is it going to let you switch it off <unk> if we do it the classical way we give it the objective of fetch the coffee i must fetch the coffee <unk> i can 't fetch the coffee[MASK] if i 'm dead so obviously the pr2 has been listening to my talk <unk> and so it says therefore i must disable my ' off ' switch [MASK] and probably taser all the other people in starbucks who might interfere with me [MASK] <unk> this seems to be inevitable right this kind of of failure mode seems to be inevitable and it follows from having a concrete definite objective <unk> [MASK] so what happens if the machine is uncertain about the objective well it reasons in a different way it says ok the human might switch me off but[MASK] only if i 'm doing something wrong well i don 't really know what wrong is but i know that i don 't want to do it so that 's the first and second principles right there[MASK] i should let the human switch me off [MASK] and in fact you can calculate the incentive that the robot has to allow the human to switch it off <unk> [MASK] and it 's directly tied to the degree of uncertainty about the underlying objective <unk> [MASK] and then when the machine[MASK] is switched off <unk> that third principle comes[MASK] <unk> it learns something about the objectives it should be pursuing because it learns that what it did wasn 't right[MASK] in fact we can with suitable use of greek symbols as mathematicians usually do <unk> we can actually prove a theorem[MASK] that says that such a robot is provably beneficial to the human you are provably better off with a machine that 's designed in this way than without it [MASK] so this is a very simple example but this is the first step in in what we 're trying to do with[MASK] now [MASK] this third principle i think is the one that you 're probably scratching your head over you 're probably thinking well you know <unk> [MASK] i behave badly <unk> i don 't want my[MASK] my robot to behave like me i sneak down in the middle of the night and take stuff from the fridge i do this and that there 's all kinds of things you don 't want the robot doing[MASK] <unk> but in fact it doesn 't quite work that way just because you[MASK] <unk> behave badly[MASK] doesn 't mean the robot is going to copy your behavior it 's going to understand your motivations[MASK] and maybe help you resist them <unk> if appropriate <unk> [MASK] but it 's still difficult what we 're trying to do in fact <unk> is to allow machines to predict for any person[MASK] and for any possible life that they could live <unk> and the lives of everybody else[MASK] <unk> which would they prefer [MASK] and there are many many difficulties involved in doing this i don 't expect that this is going to get solved very quickly <unk>[MASK] the real difficulties in fact <unk> are us <unk> [MASK] as i have already mentioned <unk> we behave badly in fact some of us are downright nasty[MASK] <unk> now the robot as i said doesn 't have to copy the behavior <unk> the robot does not have any objective of its own it 's purely altruistic <unk> [MASK] and it 's not designed just to satisfy the desires of one person the user <unk> but in fact it has to respect the preferences of everybody <unk> [MASK] so it can deal with a certain amount of nastiness and it can even understand that your nastiness for[MASK] <unk> you may take bribes as a passport official[MASK] because you need to feed your family and send your kids to school <unk>[MASK] it can understand that it doesn 't mean it 's going to steal <unk> in fact it 'll just help you send your kids to school [MASK] we are also computationally limited[MASK] <unk> a brilliant go player but he still lost so if we look at his actions he took an action that lost the game <unk> that doesn 't mean he wanted to lose [MASK] so to understand his behavior[MASK] <unk> we actually have to invert through a model of human cognition that includes our computational limitations a very complicated model <unk> [MASK] but it 's still something that we can work on understanding <unk> [MASK] probably the most difficult part from my point of view as an ai researcher is the fact that there are lots of us <unk>[MASK] <unk> and so the machine has to somehow trade off weigh up the preferences of many different people [MASK] and there are different ways to do that <unk> economists sociologists <unk> moral philosophers have understood that and we are actively looking for collaboration let 's have a look and see what happens when you get that wrong [MASK] so you can have a conversation for example with your intelligent personal assistant that might be available[MASK] in a few years ' time think[MASK] <unk> your wife called to remind you about dinner tonight and of course you 've forgotten what what dinner <unk> what are you talking about[MASK] <unk> well what am i going to do i can 't just tell him i 'm too busy don 't worry <unk> i arranged for his plane to be delayed [MASK] some kind of computer malfunction really you can do that [MASK] he sends his profound apologies and looks forward to meeting you for lunch tomorrow[MASK] <unk> 's a slight mistake going on <unk> this is clearly following my wife 's values which is happy wife happy life [MASK] it could go the other way <unk> you could come home after a hard day 's work[MASK] <unk> there are humans in south sudan who are in more urgent need than you so i 'm leaving make your own dinner [MASK] so we have to solve these problems <unk> and i 'm looking forward to working on them <unk> [MASK] there are reasons for optimism one reason is there is a massive amount of data because remember i said they 're going to read everything the human race has ever written[MASK] <unk> most of what we write about is human beings doing things[MASK] and other people getting upset about it so there 's a massive amount of data to learn from there 's also a very strong economic incentive[MASK] to get this right <unk> so imagine your domestic robot 's at home you 're late from work again[MASK] and the robot has to feed the kids and the kids are hungry and there 's nothing in the fridge <unk> and the robot[MASK] sees the cat[MASK] <unk> and the robot hasn 't quite learned the human value function properly so it doesn 't understand the sentimental value of the cat outweighs the nutritional value of the cat <unk> [MASK] so then what happens well <unk> <unk> [MASK] it happens like this deranged robot cooks kitty for family dinner[MASK] <unk> that one incident would be the end of the domestic robot industry <unk> so there 's a huge incentive to get this right[MASK] long before we reach superintelligent machines <unk> [MASK] so to summarize [MASK] i 'm actually trying to change the definition of ai so that we have provably beneficial machines <unk> and the principles are machines that are altruistic <unk> that want to achieve only our objectives <unk> [MASK] but that are uncertain about what those objectives are and will watch all of us[MASK] to learn more about what it is that we really want [MASK] and hopefully in the process we will learn to be better people <unk> thank you very much chris anderson so interesting [MASK] stuart we 're going to stand here a bit because i think they 're setting up for our next speaker a couple of questions[MASK] <unk> the idea of programming in ignorance seems intuitively really powerful <unk> as you get to <unk>[MASK] what 's going to stop a robot reading literature and discovering this idea that knowledge is actually better than ignorance and still just shifting its own goals and rewriting that[MASK] <unk> as it becomes more[MASK] correct so the the evidence is there and <unk> and it 's going to be designed to interpret it correctly it will understand for example that books[MASK] are very biased in the in the evidence they contain they only talk about kings and princes and elite white male people doing stuff <unk> so it 's a complicated problem[MASK] <unk> it learns more about our objectives it will become more and more useful to us ca and you couldn 't just boil it down to one law you know <unk> [MASK] hardwired in if any human ever tries to switch me off <unk> i comply i comply sr [MASK] absolutely not <unk> that would be a terrible idea so imagine that you have a self driving car[MASK] and you want to send your five year old off to preschool[MASK] <unk> to be able to switch off the car while it 's driving along probably not so it needs to understand[MASK] how rational and sensible the person is the more rational the person <unk> the more willing you are to be switched off if the person is completely random or even malicious then you 're less willing to be switched off ca all[MASK] right stuart can i just say i really really hope you figure this out for us thank you so much for that[MASK]